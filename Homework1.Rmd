---
title: 'HW1'
author: "ê¹€ê²½ë¯¼(20210344)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes:
  - \usepackage{kotex}
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

\newpage


#  Question 1
## Data
```{r}
library(ggplot2)
library(dplyr)

data("anscombe")
n = nrow(anscombe)
X1 = anscombe$x1
X2 = anscombe$x2
X3 = anscombe$x3
X4 = anscombe$x4
Y1 = anscombe$y1
Y2 = anscombe$y2
Y3 = anscombe$y3
Y4 = anscombe$y4
```


\newpage


## 1.1 Plot the 4 data sets (x1, y1), (x2, y2), (x3, y3), (x4, y4) using ggplot2.
### Plot (x1, y1)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```{r}

ggplot(data = anscombe) + 
  geom_point(aes(x = x1, y = y1)) + 
  xlab(bquote(x[1])) +
  ylab(bquote(y[1])) +
  ggtitle(paste0("n =", dim(anscombe %>% select(x1, y1))[1])) +
  theme(plot.title = element_text(hjust = 0.5))

```
ì„ í˜•ì‹ì„ ì í•©í•´ë„ ê´œì°®ì•„ ë³´ì¸ë‹¤.


\newpage
### Plot (x2, y2)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```{r}

ggplot(data = anscombe) + 
  geom_point(aes(x = x2, y = y2)) + 
  xlab(bquote(x[2])) +
  ylab(bquote(y[2])) +
  ggtitle(paste0("n =", dim(anscombe %>% select(x2, y2))[1])) +
  theme(plot.title = element_text(hjust = 0.5))

```
í¬ë¬¼ì„  í˜•íƒœë¼ì„œ ì„ í˜•ìœ¼ë¡œ ì í•©í•˜ëŠ” ê²ƒì€ ì ì ˆí•´ ë³´ì´ì§€ ì•ŠëŠ”ë‹¤.



\newpage
### Plot (x3, y3)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.


```{r}

ggplot(data = anscombe) + 
  geom_point(aes(x = x3, y = y3)) + 
  xlab(bquote(x[3])) +
  ylab(bquote(y[3])) +
  ggtitle(paste0("n =", dim(anscombe %>% select(x1, y1))[1])) +
  theme(plot.title = element_text(hjust = 0.5))

```
ì´ìƒì¹˜ê°€ ì¡´ì¬í•´ì„œ ì£¼ì˜í•´ì•¼ í•œë‹¤.


\newpage
### Plot (x4, y4)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```{r}

ggplot(data = anscombe) + 
  geom_point(aes(x = x4, y = y4)) + 
  xlab(bquote(x[4])) +
  ylab(bquote(y[4])) +
  ggtitle(paste0("n =", dim(anscombe %>% select(x1, y1))[1])) +
  theme(plot.title = element_text(hjust = 0.5))

```
í…Œì´í„°ê°€ ê³ ë¥´ì§€ ì•Šë‹¤. $X_4$ë¥¼ ë²”ì£¼í˜• ë³€ìˆ˜ë¡œ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ë‹¤.

\newpage
## 1.2 Fit a regression model to the data sets
### 1.2.a y1 ~ x1
ë³€ìˆ˜ ì„¤ëª…

- `Sxx1`: $S_{x_1x_1}$
- `Syy1`: $S_{y_1y_1}$
- `Sxy1`: $S_{x_1y_1}$
- `beta1_hat1`: y1 ~ x1ì—ì„œì˜ $\hat{\beta}_1$
- `beta0_hat1`: y1 ~ x1ì—ì„œì˜ $\hat{\beta}_0$

```{r}
mean.X1 = sum( X1 ) / n
mean.Y1 = sum( Y1 ) / n

Sxx1 = sum( ( X1 - mean.X1 )^2 )
Syy1 = sum( ( Y1 - mean.Y1 )^2 )
Sxy1 = sum( ( X1 - mean.X1 ) * ( Y1 - mean.Y1 ) )

beta1_hat1 = Sxy1 / Sxx1
beta0_hat1 = mean.Y1 - beta1_hat1 * mean.X1


data.frame( beta0_hat1, beta1_hat1 )
```
ì í•©ëœ ì‹ì€ $\hat{ Y }_1 = \hat{ \beta }_0 + \hat{ \beta }_0 X_1 = 3.000091 + 0.5000909	X_1$ ì´ë‹¤.


\newpage
### 1.2.b y2 ~ x2
ë³€ìˆ˜ ì„¤ëª…

- `Sxx2`: $S_{x_2x_2}$
- `Syy2`: $S_{y_2y_2}$
- `Sxy2`: $S_{x_2y_2}$
- `beta1_hat2`: y2 ~ x2ì—ì„œì˜ $\hat{\beta}_1$
- `beta0_hat2`: y2 ~ x2ì—ì„œì˜ $\hat{\beta}_0$

```{r}
mean.X2 = sum ( X2 ) / n
mean.Y2 = sum ( Y2 ) / n

Sxx2 = sum( ( X2 - mean.X2 )^2 )
Syy2 = sum( ( Y2 - mean.Y2 )^2 )
Sxy2 = sum( ( X2 - mean.X2) * ( Y2 - mean.Y2) )

beta1_hat2 = Sxy2 / Sxx2
beta0_hat2 = mean.Y2 - beta1_hat2 * mean.X2


data.frame( beta0_hat2, beta1_hat2 )
```
ì í•©ëœ ì‹ì€ $\hat{ Y }_2 = \hat{ \beta }_0 + \hat{ \beta }_0 X_2 = 3.000909 + 0.5 X_2$ ì´ë‹¤.






\newpage
### 1.2.c y3 ~ x3
ë³€ìˆ˜ ì„¤ëª…

- `Sxx3`: $S_{x_3x_3}$
- `Syy3`: $S_{y_3y_3}$
- `Sxy3`: $S_{x_3y_3}$
- `beta1_hat3`: y3 ~ x3ì—ì„œì˜ $\hat{\beta}_1$
- `beta0_hat3`: y3 ~ x3ì—ì„œì˜ $\hat{\beta}_0$

```{r}
mean.X3 = sum ( X3 ) / n
mean.Y3 = sum ( Y3 ) / n

Sxx3 = sum( ( X3 - mean.X3 )^2 )
Syy3 = sum( ( Y3 - mean.Y3 )^2 )
Sxy3 = sum( ( X3 - mean.X3) * ( Y3 - mean.Y3) )

beta1_hat3 = Sxy3 / Sxx3
beta0_hat3 = mean.Y3 - beta1_hat3 * mean.X3


data.frame( beta0_hat3, beta1_hat3 )
```
ì í•©ëœ ì‹ì€ $\hat{ Y }_3 = \hat{ \beta }_0 + \hat{ \beta }_0 X_3 = 3.002455 + 0.4997273 X_3$ ì´ë‹¤.







\newpage
### 1.2.d y4 ~ x4
ë³€ìˆ˜ ì„¤ëª…

- `Sxx4`: $S_{x_4x_4}$
- `Syy4`: $S_{y_4y_4}$
- `Sxy4`: $S_{x_4y_4}$
- `beta1_hat4`: y4 ~ x4ì—ì„œì˜ $\hat{\beta}_1$
- `beta0_hat4`: y4 ~ x4ì—ì„œì˜ $\hat{\beta}_0$

```{r}
mean.X4 = sum ( X4 ) / n
mean.Y4 = sum ( Y4 ) / n

Sxx4 = sum( ( X4 - mean.X4 )^2 )
Syy4 = sum( ( Y4 - mean.Y4 )^2 )
Sxy4 = sum( ( X4 - mean.X4) * ( Y4 - mean.Y4) )

beta1_hat4 = Sxy4 / Sxx4
beta0_hat4 = mean.Y4 - beta1_hat4 * mean.X4


data.frame( beta0_hat4, beta1_hat4 )
```
ì í•©ëœ ì‹ì€ $\hat{ Y }_4 = \hat{ \beta }_0 + \hat{ \beta }_0 X_4 = 3.001727	 + 0.4999091 X_4$ ì´ë‹¤.














\newpage
## 1.3 Compute the sample correlation for each data set. Are they the same?
```{r}
correlation1 = Sxy1 / sqrt( Sxx1 * Syy1 ) # Corr(X1, Y1)
correlation2 = Sxy2 / sqrt( Sxx2 * Syy2 ) # Corr(X2, Y2)
correlation3 = Sxy3 / sqrt( Sxx3 * Syy3 ) # Corr(X3, Y3)
correlation4 = Sxy4 / sqrt( Sxx4 * Syy4 ) # Corr(X4, Y4)

data.frame(correlation1, correlation2, correlation3, correlation4)
```
4ê°œì˜ ë°ì´í„° ì…‹ì—ì„œ sample correlationì€ ê°ê° `r correlation1`, `r correlation2`, `r correlation3`, `r correlation4`ì´ê³  ì´ë“¤ì€ ê±°ì˜ ê°™ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.
 



\newpage
## 1.4 Compute the SSE, SST and R2 value for each data set. Are they the same?

```{r}
# y1 ~x1ì—ì„œì˜ SSEì™€ SST 
SSE1 = sum( ( Y1 - beta0_hat1 - beta1_hat1 * X1)^2 ) # y1 ~x1ì—ì„œì˜ SSE
SST1 = sum( ( Y1 - mean.Y1)^2 ) # y1 ~x1ì—ì„œì˜ SST
R2_1 = 1 - SSE1 / SST1 # # y1 ~x1ì—ì„œì˜ R^2

# y2 ~x2ì—ì„œì˜ SSEì™€ SST 
SSE2 = sum( ( Y2 - beta0_hat2 - beta1_hat2 * X2)^2 ) # y2 ~x2ì—ì„œì˜ SSE
SST2 = sum( ( Y2 - mean.Y2)^2 ) # y2 ~x2ì—ì„œì˜ SST
R2_2 = 1 - SSE2 / SST2 # y2 ~x2ì—ì„œì˜ R^2

# y3 ~x3ì—ì„œì˜ SSEì™€ SST 
SSE3 = sum( ( Y3 - beta0_hat3 - beta1_hat3 * X3 )^2 ) # y3 ~x3ì—ì„œì˜ SSE
SST3 = sum( ( Y3 - mean.Y3)^2 ) # y3 ~x3ì—ì„œì˜ SST
R2_3 = 1 - SSE3 / SST3 # y3 ~x3ì—ì„œì˜ R^2

# y4 ~x4ì—ì„œì˜ SSEì™€ SST 
SSE4 = sum( ( Y4 - beta0_hat4 - beta1_hat4 * X4)^2 ) # y4 ~x4ì—ì„œì˜ SSE
SST4 = sum( ( Y4 - mean.Y4)^2 ) # y4 ~x4ì—ì„œì˜ SST
R2_4 = 1 - SSE4 / SST4 # y4 ~x4ì—ì„œì˜ R^2


data.frame(SSE1, SSE2, SSE3, SSE4)
data.frame(SST1, SST2, SST3, SST4)
data.frame(R2_1, R2_2, R2_3, R2_4)
```

\textcolor{red}{4ê°œì˜ ë°ì´í„° ì…‹ì—ì„œ SSE, SSR, $R^2$ì€ ê°ê° ëª¨ë‘ ê°™ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.}







\newpage


#  Question 3
ë°˜ì‘ë³€ìˆ˜ $Y$ëŠ” 1972ë…„ì˜ ì—¬ì„± ë…¸ë™ ì¸êµ¬ ì°¸ì—¬ìœ¨ì´ê³ , ì„¤ëª…ë³€ìˆ˜ $X$ëŠ” 1968ë…„ì˜ ì—¬ì„± ë…¸ë™ ì¸êµ¬ ì°¸ì—¬ìœ¨ì´ë‹¤. ë°ì´í„°ëŠ” ë¯¸êµ­ì˜ 19ê°œì˜ ë„ì‹œì—ì„œ ìˆ˜ì§‘ë˜ì—ˆë‹¤. ë˜í•œ $SSR = 0.0358, SSE = 0.0544$ì´ë‹¤. ê·¸ë¦¬ê³  $SST = SSE + SSR$ì„ì„ ì´ìš©í•´ ë‹¤ìŒê³¼ ê°™ì´ ë³€ìˆ˜ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆë‹¤(`n`: ë°ì´í„° ê°¯ìˆ˜, `SSE`: SSE, `SSR`: SSR, `SST`: SST, `se_beta1`: $s.e.(\hat{\beta}_1)$,`se_beta0`: $s.e.(\hat{\beta}_0)$, `beta1_hat`: $\hat{\beta}_1$, `beta0_hat`: $\hat{\beta}_0$).  
```{r}
n = 19
SSR = 0.0358
SSE = 0.0544
SST = SSE + SSR

beta1_hat = 0.656040
beta0_hat = 0.203311

se_beta1_hat = 0.1961
se_beta0_hat = 0.0976
```
ì í•©ëœ ëª¨í˜•ì€ $Y = 0.20311 + 0.65040 X$ì´ë‹¤.



## 3.1 Compute the sample variance of Y and the sample covariance betwen Y and X.
$S_{yy} = \sum_{i=1}^n (Y_i - \bar{Y})^2 = SST$ì´ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ë‹¤(`Syy`: $S_{yy}$).
```{r}
Syy = SST 
```

ê·¸ë¦¬ê³  Yì˜ í‘œë³¸ë¶„ì‚°ì€ $\dfrac{S_{yy}}{n - 1}$ì´ë‹¤. ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ë‹¤. 
```{r}
sample_variance_Y = Syy / ( n - 1 ) # Yì˜ í‘œë³¸ë¶„ì‚°
sample_variance_Y
```

ê·¸ë¦¬ê³  $s.e.(\hat{\beta}_1) = \sqrt{\hat{\sigma^2} \times \dfrac{1}{S_{xx}}} = \sqrt{\dfrac{SSE}{n-2} \times \dfrac{1}{S_{xx}}}$ì´ë‹¤. ë”°ë¼ì„œ $S_{xx} =  \sqrt{\dfrac{SSE}{n-2} \times \dfrac{1}{{s.e.(\hat{\beta}_1)}^2}}$ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤(`Sxx`: $S_{xx}$).
```{r}
Sxx = sqrt( SSE / (( n - 1 )* ( se_beta1_hat^2 )) ) 
Sxx
```

ë˜í•œ, $\hat{\beta}_1 = \frac{S_{xy}}{S_{xx}}$ì´ë¯€ë¡œ $S_{xy}$ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤(`Sxy`: $S_{xy}$).
```{r}
Sxy = Sxx * beta1_hat
Sxy
```
í‘œë³¸ê³µë¶„ì‚°ì€ $\dfrac{S_{xy}}{n - 1}$ì´ë¯€ë¡œ ê³„ì‚°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. 
```{r}
sample_covariance_Y = Sxy / ( n - 1 )
sample_covariance_Y

data.frame(sample_variance_Y, sample_covariance_Y)
```
\textcolor{red}{ê²°ê³¼ì ìœ¼ë¡œ Yì˜ í‘œë³¸ ë¶„ì‚°ì€ 0.005011111ì´ê³ , Xì™€  Y ì‚¬ì´ì˜ í‘œë³¸ ê³µë¶„ì‚°ì€ 0.01021747ì´ë‹¤.}





\newpage
## 3.2 Suppose participation rate of women in 1968 in a given city is 45%. What is the estimated participation rate of women in 1972 for the same city?

1968ë…„ì˜ ì—¬ì„± ë…¸ë™ ì¸êµ¬ ì°¸ì—¬ìœ¨ì´ $x_0 = 0.45$ì¼ ê²½ìš° 1972ë…„ì˜ ì¶”ì •ëœ ì—¬ì„± ë…¸ë™ ì¸êµ¬ ì°¸ì—¬ìœ¨ $\hat{\mu_0}$ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.          (`mu0_hat`: $\hat{\mu_0}$)
```{r}
x0 = 0.45
mu0_hat = beta0_hat + beta1_hat * x0
mu0_hat
```
\textcolor{red}{ë”°ë¼ì„œ 1972ë…„ì˜ ì¶”ì •ëœ ì—¬ì„± ë…¸ë™ ì¸êµ¬ ì°¸ì—¬ìœ¨ì€ $\hat{\mu}_0 = 0.498529$ì´ë‹¤.} 






\newpage
## 3.3 Suppose further that the mean and variance of the participantion rate of women in 1968 are 0.5 and 0.005, respectively. Construct the 95% confidence interval for the estimate in (2).
$X = x_0$ë¡œ ì£¼ì–´ì¡Œì„ ë•Œ, 1972ë…„ì˜ ì¶”ì •ëœ ë…¸ë™ì¸êµ¬ ì°¸ì—¬ìœ¨ì˜ í‘œì¤€ì˜¤ì°¨ëŠ” $s.e.(\hat{\mu}_0) = \sqrt{\hat{\sigma^2} (\frac{1}{n} + \dfrac{(x_0 - \bar{x})^2}{S_{xx}})}$ ì¸ë° $S_{xx}$ ìë¦¬ì—ëŠ” $0.005\times(n-1)$ì„ ë„£ê³  $\bar{x}$ ìë¦¬ì—ëŠ” $0.5$ë¥¼ ë„£ì–´ì„œ $\hat{\mu}_0$ì˜ 95% ì‹ ë¢°êµ¬ê°„ì„ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
```{r}
se_mu0_hat = sqrt(( SSE / ( n - 1 )) * (1/n + (x0 - 0.5)^2 / (0.005*(n-1))) )

LB_mu0 = mu0_hat - qt(0.975, df = n-2) * se_mu0_hat
UB_mu0 = mu0_hat + qt(0.975, df = n-2) * se_mu0_hat

data.frame( LB_mu0, UB_mu0 )
```
\textcolor{red}{ $\hat{\mu}_0$ì˜ $95\%$ ì‹ ë¢°êµ¬ê°„ì€ [ 0.4656392, 0.5314188]ì´ë‹¤.} 



\newpage
## 3.4 Construct the 95% confidence interval for the slope of the true regression line.
```{r}
LB_slope = beta1_hat - qt(0.975, df = n-2) * se_beta1_hat # ì‹ ë¢° í•˜í•œ
UB_slope = beta1_hat + qt(0.975, df = n-2) * se_beta1_hat # ì‹ ë¢° ìƒí•œ


data.frame( LB_slope, UB_slope )
```
\textcolor{red}{ $\hat{\beta}_1$ì˜ $95\%$ ì‹ ë¢°êµ¬ê°„ì€ [ 0.2423052, 1.069775]ì´ë‹¤.} 


\newpage
## 3.5 Test the hypothesis
 $ H_0:\beta_1 = 1$ versus $ H_\alpha:\beta_1 \neq 1$ at the 5% significance level.
```{r}
t_value = ( beta1_hat - 1 ) / se_beta1_hat 
p_value = 2 * pt(abs(t_value), df = n - 2, lower.tail = FALSE )

p_value < 0.05
```
\textcolor{red}{pê°’ì´ 0.05ë³´ë‹¤ í¬ê¸° ë•Œë¬¸ì— ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•  ìˆ˜ ì—†ë‹¤.}



\newpage
## 3.6 Compute the R2 for this simple linear regression.

```{r}
R_square = SSR / SST # R^2
R_square
```
\textcolor{red}{ $R^2$ì€ 0.3968958ì´ë‹¤.}

\newpage
## 7. If ğ‘‹ and ğ‘Œ were reversed in the above regression, what would you expect R2 to be?
\textcolor{red}{$X$ì™€ $Y$ê°€ ë°”ë€Œë”ë¼ë„ $R^2$ëŠ” ë˜‘ê°™ì„ ê²ƒ ê°™ë‹¤.} ë‹¨ìˆœì„ í˜•íšŒê·€ì—ì„œëŠ” $R^2$ê°€ ë²ˆì‘ë³€ìˆ˜ì™€ ì˜ˆì¸¡ë³€ìˆ˜ ì‚¬ì´ì˜ ìƒê´€ê³„ìˆ˜ì˜ ì œê³±ì´ê¸° ë–„ë¬¸ì— Xì™€ Yê°€ ë°”ë€Œì–´ë„ ê°™ì„ ê²ƒì´ë‹¤ë‹¤ .
$$
SSE = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2 = \sum_{i=1}^n (\hat{\beta}_0 + \hat{\beta}_1 x_i  - (\hat{\beta}_0 + \hat{\beta}_1 \bar{x}))^2 = \hat{\beta}_1^2 S_{xx}  
$$
$$
R^2 = \dfrac{SSE}{SST} = \dfrac{\hat{\beta}_1^2 S_{xx}}{S_{yy}} = \dfrac{S_{xy}^2}{S_{xx}^2}\dfrac{S_{xx}}{S_{yy}} = \dfrac{S_{xy}^2}{S_{xx} S_{yy}} = {Corr(X,Y)}^2
$$









